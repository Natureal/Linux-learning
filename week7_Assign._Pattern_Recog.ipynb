{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pattern Recognition Week 7 Assignment Report"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Discuss the similarity and difference between Kalman filter and hidden Markov Model (foward algorithm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Difference\n",
    "### (1) The biggest difference is that an HMM has a finite-number of state while the states of a kalman filter are continuous variables.\n",
    "### (2) Different evolution method on the Markovian chain, an HMM transit its states by probability transition matrix whereas in a kalman filter, the state evolves based on state transition matrix as well as a Gaussian noise.\n",
    "### (3) In HMMs, hidden states are able to generate observable signals while in kalman filters, states are unobservable due to Gaussian noise.\n",
    "## Similarity\n",
    "### (1) Both of them are first order Markovian processes. The k-th state depends on the (k-1)-th state only.\n",
    "### (2) The observations in both follow Gaussian distribution."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----------------------------------------------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Explain how to estimate the coefficients a1 and a2 of the auto-regressive (AR) model, $y_{t} = a_{1}y_{t-1}+a_{2}y_{t-2}+ \\epsilon_{t}$, from observed sequence data $y_{1},y_{2},...,y_{T} \\in R.$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Maximum likelihood estimation\n",
    "### (1) We assume that $\\epsilon_{t}$ is independent and following a $N(0,\\sigma^{2})$ distribution. Given $y_{1},y_{2},...,y_{T}$, we want to estimate the parameter set ($a_{1},a_{2}$, $\\sigma^{2}$).\n",
    "### (2) First, we need to find the likelihood function for this AR(2) model:\n",
    "### $p(y_{1},y_{2},y_{3},...,y_{n}|a_{1},a_{2},\\sigma^{2}) = p(y_{1},y_{2},...,y_{T}|a_{1},a_{2},\\sigma^{2}) \\prod_{t=T+1}^{n} p(y_{t}|y_{t-1},y_{t-2},a_{1},a_{2},\\sigma^{2}) $\n",
    "### here the first T observations are already known. Therefore, the equation could be rewritten as: $p(y_{1},y_{2},y_{3},...,y_{n}|a_{1},a_{2},\\sigma^{2}) = \\prod_{t=T+1}^{n} p(y_{t}|y_{t-1},y_{t-2},a_{1},a_{2},\\sigma^{2}) $\n",
    "### At the meantime, y also follows the normal distribution, so \n",
    "$p(y_{t}|y_{t-1},y_{t-2},a_{1},a_{2},\\sigma^{2}) = \\frac{1}{\\sqrt{2 \\pi}\\sigma} exp(-\\frac{(y_{t}-a_{1}y_{t-1}-a_{2}y_{t-2})^{2}}{2\\sigma^{2}})$ \n",
    "### Thus, the log-likelihood function can be:\n",
    "$L = \\sum_{T+1}^{n} log(\\frac{1}{\\sqrt{2 \\pi}\\sigma} exp(-\\frac{(y_{t}-a_{1}y_{t-1}-a_{2}y_{t-2})^{2}}{2\\sigma_{2}}))=-\\frac{n-T}{2}log(2\\pi)-\\frac{n-T}{2}log(\\sigma^{2})-\\frac{1}{2\\sigma^{2}}\\sum_{t=T+1}^{n}(y_{t}-a_{1}y_{t-1}-a_{2}y_{t-2})^{2}$\n",
    "### Calculate the derivatives and let them equal to 0, we can get the result of estimation:\n",
    "$a_{1} = \\frac{\\sum_{t=T+1}^{n}y_{t}y_{t-1} - a_{2}\\sum_{t=T+1}^{n}y_{t-1}y_{t-2}}{\\sum_{t=T+1}^{n}y_{t-1}^{2}}$<br>\n",
    "$a_{2} = \\frac{\\sum_{t=T+1}^{n}y_{t}y_{t-2} - a_{1}\\sum_{t=T+1}^{n}y_{t-2}y_{t-1}}{\\sum_{t=T+1}^{n}y_{t-2}^{2}}$<br>\n",
    "$\\sigma^{2}=\\frac{1}{n-T}\\sum_{t=T+1}^{n}(y_{t}-a_{1}y_{t-1}-a_{2}y_{t-2})^{2}$"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
